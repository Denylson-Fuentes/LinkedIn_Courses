{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model only accept vectors as input\n",
    "\n",
    "we are required to convert the text string to numbers and vectorize\n",
    "them \n",
    "\n",
    "Two very common vectorizing:\n",
    "    One-hot encoding\n",
    "    Unique number encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "create a zero vector of size of array equal to the vocab and add\n",
    "1 to the indices corresponding to the word\n",
    "\n",
    "the approach is inefficient, The type of vector is sparse(most indices are zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Number Encoding\n",
    "\n",
    "encode each word with a unique number\n",
    "\n",
    "Drawback:\n",
    "    the integer encoding doesn't capture relationships\n",
    "    the lack of relationship makes it hard to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embedding\n",
    " a dense vector of floating points values with length of the \n",
    "    vector provided as a parameter\n",
    "\n",
    "no manual assignment of values, encoding are trainable params\n",
    "    that are learned furing model training\n",
    "\n",
    "embeddings offers an efficient and dense vector to \n",
    "    represent the relationship between words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-polymer",
   "metadata": {},
   "source": [
    "# Word Embeddings for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "grand-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "domestic-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Denylson\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69e82e20ec4e109285ce4cc6188d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b09661f22c24310ad894413f1a4f672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Denylson\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#We will get both metadata and labels to them\n",
    "data, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earlier-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "atomic-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, label in train_data:\n",
    "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "for sent, label in test_data:\n",
    "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "distant-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "actual-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "emb_size = 32\n",
    "max_length = 150\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<oov>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "obvious-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seqs, maxlen = max_length, truncating = trunc_type)\n",
    "\n",
    "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seqs, maxlen = max_length, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "resident-chorus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  11  26  75 571   6 805   1 313 106  19  12   7 629 686   6   4\n",
      "   1   5 181 584  64   1 110   1   3   1  21   2   1   3 258  41   1   4\n",
      " 174 188  21  12   1  11   1   1  86   2  20  14   1   2 112 940  14   1\n",
      "   1 548   3 355 181 466   6 591  19  17  55   1   5  49  14   1  96  40\n",
      " 136  11 972  11 201  26   1 171   5   2  20  19  11 294   2   1   5  10\n",
      "   3 283  41 466   6 591   5  92 203   1 207  99 145   1  16 230 332  11\n",
      "   1 384  12  20  31  30]\n",
      "??????????????????????????????????????ihavebeenknowntofall<oov>duringfilmsbutthisisusuallyduetoa<oov>ofthingsincludingreally<oov>being<oov>and<oov>onthe<oov>andhavingjust<oov>alothoweveronthis<oov>i<oov><oov>becausethefilmwas<oov>theplotdevelopmentwas<oov><oov>slowandboringthingsseemedtohappenbutwithno<oov>ofwhatwas<oov>themorwhyiadmitimayhave<oov>partofthefilmbutiwatchedthe<oov>ofitandeverythingjustseemedtohappenofitsown<oov>withoutanyreal<oov>foranythingelsei<oov>recommendthisfilmatall\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key)  for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ''.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_padded[1])\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-plant",
   "metadata": {},
   "source": [
    "# Defining a Nueral Network with Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stainless-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 28806     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 60,813\n",
      "Trainable params: 60,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, emb_size, input_length = max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prime-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.5148 - accuracy: 0.7268 - val_loss: 0.3976 - val_accuracy: 0.8201\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3384 - accuracy: 0.8567 - val_loss: 0.4132 - val_accuracy: 0.8143\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2384 - accuracy: 0.9101 - val_loss: 0.4776 - val_accuracy: 0.7943\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1358 - accuracy: 0.9571 - val_loss: 0.6153 - val_accuracy: 0.7876\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0654 - accuracy: 0.9850 - val_loss: 0.7463 - val_accuracy: 0.7820\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0301 - accuracy: 0.9952 - val_loss: 0.8702 - val_accuracy: 0.7827\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0140 - accuracy: 0.9978 - val_loss: 0.9742 - val_accuracy: 0.7816\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 1.0799 - val_accuracy: 0.7786\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 1.1731 - val_accuracy: 0.7782\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.2553 - val_accuracy: 0.7786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x210c6ef4ac8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.fit(\n",
    "    train_padded,\n",
    "    train_labels,\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (test_padded, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-summer",
   "metadata": {},
   "source": [
    "# Deriving weight from  Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "declared-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32)\n",
      "[[ 0.03659494  0.00098447 -0.00871876 ...  0.01547277 -0.01957596\n",
      "  -0.02067693]\n",
      " [ 0.03770637 -0.00848457 -0.02598854 ... -0.04054574 -0.00193059\n",
      "  -0.03420943]\n",
      " [ 0.03052573  0.04776368  0.02018204 ...  0.04097027  0.01607944\n",
      "   0.00581276]\n",
      " ...\n",
      " [ 0.15759665 -0.20130087  0.09648244 ...  0.05359149  0.00288965\n",
      "  -0.13382825]\n",
      " [ 0.11952089 -0.04653597 -0.15234673 ...  0.10648727 -0.15857537\n",
      "   0.0440921 ]\n",
      " [ 0.02776677 -0.20963357 -0.1583334  ...  0.10204659 -0.15585104\n",
      "  -0.0685412 ]]\n"
     ]
    }
   ],
   "source": [
    "l1 = model.layers[0]\n",
    "weights = l1.get_weights()[0]\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-karma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wound-mayor",
   "metadata": {},
   "source": [
    "# Part 2 (Projecting embeddings on tensorflow projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "impressive-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "vectors = io.open('vectors.tsv', 'w', encoding = 'utf-8')\n",
    "\n",
    "meta = io.open('meta.tsv', 'w', encoding= 'utf-8')\n",
    "\n",
    "#write each word and its correspoding metadata\n",
    "for index in range(1, vocab_size):\n",
    "    word = reverse_word_index[index]\n",
    "    embeddings = weights[index]\n",
    "    meta.write(word + '\\n')\n",
    "    vectors.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-packet",
   "metadata": {},
   "source": [
    "# Classifying News headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('./xl.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = list(data['headline'])\n",
    "labels = list(data['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Parameters\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "emb_size = 32\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<oov>'\n",
    "\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data\n",
    "\n",
    "training_sentences = headlines[0:training_size]\n",
    "testing_sentences = headlines[training_size:]\n",
    "\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_senteces)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to np.array to work with Tensorflow 2.x\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, emb_size, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    training_padded, \n",
    "    training_labels,\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (testing_padded, testing_labels)\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_' + string])\n",
    "    plt.legend([string, 'val_' + string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying a new sentence\n",
    "\n",
    "sentence = ['the baby boy fears spiders in the garden in the garden might be real', 'game of thrones season finale showing this sunday night']\n",
    "\n",
    "#prepare the senquences of the sentences in question\n",
    "sequences = tokenizer.text_to_sequences(sentence)\n",
    "padded_seqs = pad_sequences(sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "print(model.predict(padded_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-fifteen",
   "metadata": {},
   "source": [
    "# Text Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "massive-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load('imdb_reviews', with_info = True, as_supervised = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "powered-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "train_senteces = []\n",
    "test_senteces = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mechanical-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in train_data:\n",
    "    train_sentences.append(str(sentence.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "for sentence, label in test_data:\n",
    "    test_sentences.append(str(sentence.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adverse-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "opponent-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "vocab_size = 10000\n",
    "emb_size = 50\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<oov>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "furnished-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = '<oov>')\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#training sequences and labels\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seq, maxlen = max_length, truncating = trunc_type)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seq, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "certified-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, emb_size, input_length = max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "conditional-soldier",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 50001\n  y sizes: 25000\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-fd93c2020f7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_padded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 50001\n  y sizes: 25000\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_padded,\n",
    "    train_labels,\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (test_padded, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "commercial-python",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0ba8a494dfed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mplot_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0ba8a494dfed>\u001b[0m in \u001b[0;36mplot_metric\u001b[1;34m(history, metric)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_' + metric])\n",
    "    plt.legend([string, 'val_'+ metric])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n",
    "    \n",
    "plot_metric(history, 'accuracy')\n",
    "plot_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-chile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
